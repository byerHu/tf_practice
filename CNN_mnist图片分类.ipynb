{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byerHu/tf_practice/blob/master/CNN_mnist%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Zjti8AetSJ-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "82e5cec3-1960-4861-9017-c9d211da987d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "# 加载数据\n",
        "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
        "\n",
        "# 每个批次的大小\n",
        "batch_size = 100\n",
        "\n",
        "# 计算一个有多少个批次\n",
        "n_batch = mnist.train.num_examples // batch_size\n",
        "\n",
        "# 初始化权值\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "# 初始化偏置值\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1,shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "# 卷积层\n",
        "def conv2d(x,W):\n",
        "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
        "\n",
        "# 池化层\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-61e869375e3e>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "81vHMp4qR5HJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "43abb721-9171-492b-ea6e-e2be72d5276a"
      },
      "cell_type": "code",
      "source": [
        "# 定义两个placeholder\n",
        "x = tf.placeholder(tf.float32,[None,784])\n",
        "y = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "# 改变x的格式转为4D的向量[batch,in_height,in_width,in_channels]\n",
        "x_image = tf.reshape(x,[-1,28,28,1]) \n",
        "\n",
        "# 初始化第一个卷积层的权值和偏置\n",
        "W_conv1 = weight_variable([5,5,1,32]) # 采用5*5的采样窗口，32个卷积核从1个平面抽取特征\n",
        "b_conv1 = bias_variable([32]) # 每一个卷积核对应一个偏置值\n",
        "\n",
        "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1) # 进行max池化\n",
        "\n",
        "# 初始化第二个卷积层的权值和偏置\n",
        "W_conv2 = weight_variable([5,5,32,64]) # 5*5的采样窗口，64个卷积核从32个平面抽取特征\n",
        "b_conv2 = bias_variable([64]) # 每个卷积核对应一个偏置值\n",
        "\n",
        "# 把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu函数\n",
        "h_conv2  = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "# 28*28的图片第一次卷积之后还是28*28,第一次池化之后变为了14*14\n",
        "# 第二次卷积后为14*14，第二次池化后变为了7*7\n",
        "# 经过上面的操作后得到了64张7*7平面\n",
        "\n",
        "# 初始化第一个全连接层的权值\n",
        "W_fc1 = weight_variable([7*7*64,1024]) # 上一层7*7*64,全连接层有1024个神经元\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "# 把池化层2的输出扁平化为1维\n",
        "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
        "# 求第一个全连接层的输出\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
        "\n",
        "# keep_prob用来表示神经元的输出概率\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
        "\n",
        "# 初始化第二个全连接层\n",
        "W_fc2 = weight_variable([1024,10])\n",
        "b_fc2  = bias_variable([10])\n",
        "\n",
        "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
        "\n",
        "# 交叉熵代价函数\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
        "\n",
        "# 最小化损失\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "# 将结果放入一个布尔列表当中\n",
        "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) \n",
        "\n",
        "# 求准确率\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(21):\n",
        "        for batch in range(n_batch):\n",
        "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
        "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
        "        print(\"Iter \"+str(epoch)+\", Testing Accuracy: \"+str(acc))\n",
        "        "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, Testing Accuracy: 0.9534\n",
            "Iter 1, Testing Accuracy: 0.9714\n",
            "Iter 2, Testing Accuracy: 0.9774\n",
            "Iter 3, Testing Accuracy: 0.9815\n",
            "Iter 4, Testing Accuracy: 0.9838\n",
            "Iter 5, Testing Accuracy: 0.9856\n",
            "Iter 6, Testing Accuracy: 0.9852\n",
            "Iter 7, Testing Accuracy: 0.9865\n",
            "Iter 8, Testing Accuracy: 0.9872\n",
            "Iter 9, Testing Accuracy: 0.9907\n",
            "Iter 10, Testing Accuracy: 0.9895\n",
            "Iter 11, Testing Accuracy: 0.9885\n",
            "Iter 12, Testing Accuracy: 0.9879\n",
            "Iter 13, Testing Accuracy: 0.9888\n",
            "Iter 14, Testing Accuracy: 0.9893\n",
            "Iter 15, Testing Accuracy: 0.9905\n",
            "Iter 16, Testing Accuracy: 0.99\n",
            "Iter 17, Testing Accuracy: 0.9913\n",
            "Iter 18, Testing Accuracy: 0.9905\n",
            "Iter 19, Testing Accuracy: 0.9911\n",
            "Iter 20, Testing Accuracy: 0.9918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F9ROMr3SSqMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}